{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environmental and Social-Economic Assessment (ESA) Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On 1 May 2020, the Canadian Energy Regulator (CER) published the [ESD Data Bank](https://apps.cer-rec.gc.ca/REGDOCS/Home/Index/FAKE), an interactive tool that allows users to visualize, download, and share ESA data from applications filed in support of federally regulated pipeline projects. The tools contains tables and figures from 37 pipeline projects submitted to the CER between 2003 and 2019 (output files). Data was extracted from 1,902 PDF documents available from the CER's public repository called [REGDOCS](https://apps.cer-rec.gc.ca/REGDOCS).  <br>\n",
    "\n",
    "To download individual tables (in CSV and JPG format) and figures (in JPG) format, see the [ESD Data Bank](https://apps.cer-rec.gc.ca/REGDOCS/Home/Index/FAKE) Data Bank online tool. The [ESA Data Bank](https://apps.cer-rec.gc.ca/REGDOCS/Home/Index) is an interactive tool that allows users to visualize, download, and share the Canada Energy Regulator’s (CER) ESA data from applications filed in support of federally regulated pipeline projects and related facilities. <br>\n",
    "\n",
    "This repo contains several python functions that create the ESA dataset and the figure and table output files. Data is extracted from PDF files submitted by pipeline companies. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the Code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebooks covers the code for the first step to re-create the ESA Data Bank dataset. In the first step, we focus on the data extraction and data preparation for the 1902 PDF Files. <br>\n",
    "\n",
    "“Index of PDFs for Major Projects with ESAs” (Index0) is already created which contains the list of the PDF files submitted for the. In this notebook we try to cover the following steps: <br>\n",
    "\n",
    "1.\tScrape PDF File<br>\n",
    "\n",
    "2.\tRotate the PDF Files <br>\n",
    "\n",
    "3.  Convert PDF to Pickled Files <br>\n",
    "\n",
    "4.\tConvert rotated PDF Files to rotated Pickled Files\n",
    "\n",
    "5.  Extract PDF Metadata <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing the Required Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing Python standard libraries\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import multiprocessing\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing custom libraries built by CER DDA team  \n",
    "import scraper\n",
    "import rotate_pdfs\n",
    "import pickles_functions_mp\n",
    "import pdf_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Input Files (Index of PDFs for Major Projects with ESAs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Index0_path = os.path.realpath('..\\\\..') + \"\\\\Input_Files\\\\Index_of_PDFs_for_Major_Projects_with_ESAs.csv\"\n",
    "\n",
    "Index0 = pd.read_csv(Index0_path, index_col = 0)\n",
    "Index0.head()\n",
    "Index0.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Scrape PDF Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are downloading the PDF Files using the downloadable link provided in the Index0 Dataframe and saving the files to our desktop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inorder to have a faster trial demo, we are limiting the number of files to 10 \n",
    "Index0 = Index0.head(10)\n",
    "len(Index0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = scraper.file_scraper(os.path.realpath('..\\\\..'), Index0)\n",
    "print(\"{} Files were downloaded from {} URL links\".format(count, len(Index0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Rotate the PDF Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some pages in the PDF files for the ESA projects were rotated by 90 degrees. Extraction of data from those files can be extremely time taking. Hence, this function was used to keep the rotated PDF files in a seperate folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = rotate_pdfs.rotate_pdf(os.path.realpath('..\\\\..'), Index0)\n",
    "print(\"{} Files were rotaed successfully rotated\".format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Convert PDFs to Pickled Files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are using the pickle library which implements binary protiocals for serializing and de-serializing on the python object of the PDF files and converts teh PDF files into pickled files. The pickle data format uses a relatively compact binary representation, allowing faster processing of the files with a reduced failure rate. We have implemented multiprocessing and sequential processing for this step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of full paths to pdfs\n",
    "pdf_folder_path = os.path.realpath('..\\\\..') + '\\\\Data_Files\\\\PDFs\\\\'\n",
    "\n",
    "subset_list_pdf_full = [pdf_folder_path\n",
    "                        + x.split('\\\\')[-1] for x in glob.glob(pdf_folder_path + '*.pdf')]\n",
    "\n",
    "# Directory where the output pickle files are saved\n",
    "pkl_folder_path = os.path.realpath('..\\\\..') + '\\\\Data_Files\\\\Pickle_Files\\\\'\n",
    "# prepare arguments for multiprocessing\n",
    "args = pickles_functions_mp.get_argument(subset_list_pdf_full, pkl_folder_path)\n",
    "\n",
    "# timing the process-start\n",
    "starttime = time.time()\n",
    "\n",
    "# #sequential\n",
    "# for arg in args:\n",
    "#     try:\n",
    "#         pickles_functions_mp.pickle_pdf_xml(arg)\n",
    "#     except Exception:\n",
    "#         #print(\"exception was raised for {}\".format(arg))\n",
    "#         pass\n",
    "\n",
    "# multiprocessing\n",
    "pool = multiprocessing.Pool()\n",
    "pool.map(pickles_functions_mp.pickle_pdf_xml, args)\n",
    "pool.close()\n",
    "#time ends and dellta displayed\n",
    "print('That took {} seconds'.format(time.time() - starttime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Convert rotated PDF Files to rotated TIKA Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for the rotated pages of the PDF Files will not be extrated correctly unless the PDF files are rotated too. Hence, in this step, we are pickling the rotated PDF files too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list of full paths to roatted pdfs\n",
    "pdf_folder_path = os.path.realpath('..\\\\..') + '\\\\Data_Files\\\\PDFs_Rotated\\\\'\n",
    "\n",
    "subset_list_pdf_full = [pdf_folder_path\n",
    "                        + x.split('\\\\')[-1] for x in glob.glob(pdf_folder_path + '*.pdf')]\n",
    "\n",
    "# Directory where the output pickle files are saved\n",
    "pkl_folder_path = os.path.realpath('..\\\\..') + '\\\\Data_Files\\\\Pickle_Files_Rotated\\\\'\n",
    "# prepare arguments for multiprocessing\n",
    "args = pickles_functions_mp.get_argument(subset_list_pdf_full, pkl_folder_path)\n",
    "\n",
    "# timing the process-start\n",
    "starttime = time.time()\n",
    "\n",
    "# #sequential\n",
    "# for arg in args:\n",
    "#     try:\n",
    "#         pickles_functions_mp.pickle_pdf_xml(arg)\n",
    "#     except Exception:\n",
    "#         #print(\"exception was raised for {}\".format(arg))\n",
    "#         pass\n",
    "\n",
    "# multiprocessing\n",
    "pool = multiprocessing.Pool()\n",
    "pool.map(pickles_functions_mp.pickle_pdf_xml, args)\n",
    "pool.close()\n",
    "#time ends and dellta displayed\n",
    "print('That took {} seconds'.format(time.time() - starttime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Extracting PDF Metadata  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are trying to extract some useful metadata from these PDF files.  which  from the PDF files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identify ESA categories for the PDF files  \n",
    "Index1 = pdf_metadata.pdf_categorize(Index0_path, Index0)\n",
    "\n",
    "# Identify the PDF File size \n",
    "Index1 = pdf_metadata.pdf_size(os.path.realpath('..\\\\..'), Index1)\n",
    "\n",
    "# Identify the number of pages in the PDF file\n",
    "Index1 = pdf_metadata.pdf_pagenumbers(os.path.realpath('..\\\\..'), Index1)\n",
    "\n",
    "# Identify if Outline (or TOC) is present in the PDF file or not\n",
    "Index1 = pdf_metadata.get_outline_present(os.path.realpath('..\\\\..'), Index1)\n",
    "\n",
    "Index1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Index1.to_csv(os.path.realpath('..\\\\..') + '\\\\Output_Files\\\\Index 1 - PDFs for Major Projects with ESAs.csv', index = False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
