{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copying tables from F or G drive to the esa-ees folder to put the files in production\n",
    "\n",
    "This notebook contains the necessary code to copy the ESA CSV files as well as the index files into the esa-ees folder. Once the files are in esa-ees, IT will take the files and copy them into the production / test folders.\n",
    "\n",
    "Part of the code organizes all the CSVs into their respective project folder. The files are then zipped into a zip file to make it easier for users to download the files.\n",
    "\n",
    "**What's missing?** There needs to be code added to bundle the files together. Personally, I would use a new duplicate folder of the csv files and test how you can bundle the files. Once you've bundled the files correctly, rearrange the code in this notebook as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shutil # for copying files\n",
    "from tqdm import tqdm # progress bar for for-loops\n",
    "from zipfile import ZipFile # for creating zip files\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath to the English and French index files\n",
    "ENG_index_filepath = 'F:/Environmental Baseline Data/Version 4 - Final/Indices/ESA_website_ENG.csv'\n",
    "FRA_index_filepath = 'F:/Environmental Baseline Data/Version 4 - Final/Indices/ESA_website_FRA.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caution: Following code is to the delete files in esa-ees.\n",
    "\n",
    "Use this code when your are copying new files to esa-ees and you want to avoid creating duplicates with different filenames (or folder names).\n",
    "\n",
    "**Skip if you don't want to delete files in esa-ees.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 38/38 [16:16<00:00, 25.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# df = pd.read_csv(ENG_index_filepath)\n",
    "# df['Download folder name'] = r'//dweb5/esa-ees/' + df['Download folder name'] + '/*'\n",
    "# unique_folders = df['Download folder name'].unique()\n",
    "\n",
    "# for x in tqdm(range(0, len(unique_folders))):\n",
    "#     folder_name = unique_folders[x]\n",
    "#     files = glob.glob(folder_name)\n",
    "#     for f in files:\n",
    "#         os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying index files with pandas to be able to copy files to esa-ees folder (the destination folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Loading index file of all tables\n",
    "df = pd.read_csv(ENG_index_filepath)\n",
    "\n",
    "# Grabbing project folder names and replacing with folder paths in esa-ees\n",
    "df['Download folder name'] = r'//dweb5/esa-ees/' + df['Download folder name'] + '/'\n",
    "\n",
    "# Creating new \n",
    "df['csv_url'] = df[\"CSV Download URL\"].str.split('/|_')\n",
    "\n",
    "# Remove all rows for figures so that we are only moving tables\n",
    "df = df[df['Content Type'] == 'Table']\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Adding source and destination filepaths to the dataframe\n",
    "source_folder_path = 'F:/Environmental Baseline Data/Version 4 - Final/all_csvs_cleaned/'\n",
    "source_filenames = []\n",
    "dest_filenames = []\n",
    "for x in tqdm(range(0, len(df))):\n",
    "    source_file_name = source_folder_path + df['csv_url'].iloc[x][-3] + '_' + df['csv_url'].iloc[x][-2] + '_lattice-v_' + df['csv_url'].iloc[x][-1][0] + '.csv'\n",
    "    source_filenames.append(source_file_name)\n",
    "    dest_file_name = df['Download folder name'].iloc[x] + df['csv_url'].iloc[x][-3] + '_' + df['csv_url'].iloc[x][-2] + '_' + df['csv_url'].iloc[x][-1][0] + '.csv'\n",
    "    dest_filenames.append(dest_file_name)\n",
    "\n",
    "df['csv_path'] = pd.Series(source_filenames)\n",
    "df['Download folder name'] = pd.Series(dest_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking a cell to see what the paths look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:/Environmental Baseline Data/Version 4 - Final/all_csvs_cleaned/1059614_14_lattice-v_1.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['csv_path'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'//dweb5/esa-ees/nrthmntn/1059614_14_1.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Download folder name'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copying the files from the source folder to the destination folder (esa-ees):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 28891/28891 [43:24<00:00, 11.09it/s]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "exceptions = []\n",
    "\n",
    "for x in tqdm(range(0, len(df))):\n",
    "    try: \n",
    "        # copying the files into the destination folder\n",
    "        shutil.copy(df['csv_path'].iloc[x], df['Download folder name'].iloc[x])\n",
    "    except:\n",
    "        pass\n",
    "        exceptions.append(df['csv_path'].iloc[x])\n",
    "#     print(str(x) + ' of ' + str(len(df)) + ' files moved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking to see if we were able to move every single file\n",
    "# 'exceptions' will be empty if so\n",
    "exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the index files as individual zip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENG_website_filename = 'ESA_website_ENG.csv'\n",
    "FRA_website_filename = 'ESA_website_FRA.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: The next coding block may need to include a duplicate for the French index file. Please contact Angelsea for info on how she would like the files to be saved. This is likely the case so I've duplicated the code for the French file as well. Delete if unnecessary.**\n",
    "\n",
    "Another thing, you may need to zip some other files as well, I'm unsure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Loading ENG index file\n",
    "df = pd.read_csv(ENG_index_filepath)\n",
    "\n",
    "# Removing rows for the figures\n",
    "df = df[df['Content Type'] == 'Table']\n",
    "\n",
    "# destination path\n",
    "dest_path = r'//dweb5/esa-ees/'\n",
    "\n",
    "df.to_csv(dest_path + ENG_website_filename)\n",
    "\n",
    "# Create a ZipFile object\n",
    "zipObj = ZipFile('DLD-ndc.zip', 'w')\n",
    "\n",
    "# Add multiple files to the zip by using zipObj.write(...) for each file/folder you want to include in the zip folder.\n",
    "zipObj.write(dest_path + ENG_website_filename)\n",
    "                 \n",
    "# Close the Zip File\n",
    "zipObj.close()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a csv and excel file the download paths for each csv (separated by folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 38/38 [00:58<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "# English files\n",
    "\n",
    "%%time\n",
    "\n",
    "# Loading ENG index file\n",
    "df = pd.read_csv(ENG_index_filepath)\n",
    "\n",
    "# Removing rows for the figures\n",
    "df = df[df['Content Type'] == 'Table']\n",
    "\n",
    "unique_folders = df['Download folder name'].unique()\n",
    "download_folder_path_df = r'//dweb5/esa-ees/' + unique_folders + '/'\n",
    "\n",
    "exceptions = []\n",
    "\n",
    "for x in tqdm(range(0, len(unique_folders))):\n",
    "    try:\n",
    "        # Get name of the folder of a project\n",
    "        folder_name = unique_folders[x]\n",
    "        \n",
    "        # Filter out all rows that aren't associated with project folder x\n",
    "        folder_df = df[df['Download folder name'] == folder_name]\n",
    "        \n",
    "        # Save download path csv and excel files\n",
    "        folder_df.to_csv(download_folder_path_df[x] + folder_name + '_ENG.csv')\n",
    "        folder_df.to_excel(download_folder_path_df[x] + folder_name + '_ENG.xlsx')\n",
    "    except:\n",
    "        pass\n",
    "        exceptions.append(df['Download folder name'].iloc[x])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 37/37 [00:58<00:00,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 59.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Same as above, but for the French files\n",
    "\n",
    "%%time\n",
    "\n",
    "df = pd.read_csv(FRA_index_filepath)\n",
    "df = df[df['Type de contenu'] == 'Tableau']\n",
    "dossiers_uniques = df['Télécharger le nom du dossier'].unique()\n",
    "download_folder_path_df = r'//dweb5/esa-ees/' + dossiers_uniques + '/'\n",
    "\n",
    "exceptions = []\n",
    "\n",
    "for x in tqdm(range(0, len(dossiers_uniques))):\n",
    "    try:\n",
    "        # Get name of the folder of a project\n",
    "        nom_du_dossier = dossiers_uniques[x]\n",
    "        \n",
    "        # Filter out all rows that aren't associated with project folder x\n",
    "        dossier_df = df[df['Télécharger le nom du dossier'] == nom_du_dossier]\n",
    "        \n",
    "        # Save download path csv and excel files\n",
    "        dossier_df.to_csv(download_folder_path_df[x] + nom_du_dossier + '_FRA.csv')\n",
    "        dossier_df.to_excel(download_folder_path_df[x] + nom_du_dossier + '_FRA.xlsx')\n",
    "    except:\n",
    "        pass\n",
    "        exceptions.append(df['Télécharger le nom du dossier'].iloc[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a zip file of each individual project folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = pd.read_csv(ENG_index_filepath)\n",
    "unique_folders = df['Download folder name'].unique()\n",
    "folder_paths = r'//dweb5/esa-ees/' + unique_folders\n",
    "\n",
    "exceptions = []\n",
    "\n",
    "for x in tqdm(range(0, len(unique_folders))):\n",
    "    try:\n",
    "        shutil.make_archive(folder_paths[x], 'zip', folder_paths[x])\n",
    "    except:\n",
    "        pass\n",
    "        exceptions.append(folder_paths[x])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
