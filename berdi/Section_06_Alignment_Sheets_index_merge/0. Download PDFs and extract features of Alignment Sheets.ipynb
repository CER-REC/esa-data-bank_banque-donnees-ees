{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import multiprocessing\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import requests\n",
    "import glob\n",
    "import pickle\n",
    "import fitz\n",
    "from string import digits\n",
    "# from Codes.Section_01_Data_Extraction_Preparation.file_preparation import download_file, rotate_pdf, pickle_pdf_xml\n",
    "# from Codes.Section_01_Data_Extraction_Preparation.pdf_metadata import get_pdf_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"c:\\\\Users\\\\t1nipun\\\\Desktop\\\\BERDI\\\\esa-data-bank_banque-donnees-ees\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download all PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read all the names of the PDFs to be downloaded\n",
    "path = os.getcwd()\n",
    "Index0_path = path + '\\\\data\\\\raw\\\\index_for_projects\\\\Phase3_Index_of_PDFs_for_Major_Projects_with_ESAs.csv'\n",
    "\n",
    "Index0 = pd.read_csv(Index0_path, encoding= 'unicode_escape')\n",
    "print(\"Number of PDF Files to be downloaded: \", len(Index0))\n",
    "\n",
    "Index0 = Index0.rename(columns={'Application Short Name': 'App_Short_Name'})\n",
    "\n",
    "Index0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to download the PDFs\n",
    "\n",
    "def download_file(path, Index0):\n",
    "    count = 0\n",
    "    error_urls = []\n",
    "    error_dataIDs = []\n",
    "    dataID = \"\"\n",
    "\n",
    "    # Iterating each row in the Index0 dataframe\n",
    "    for index, row in Index0.iterrows():\n",
    "        try:\n",
    "            dataID = row['DataID']\n",
    "            download_url = 'http://docs2.cer-rec.gc.ca/ll-eng/llisapi.dll?func=ll&objId=' + str(dataID) + '&objaction=download&viewType=1'\n",
    "            r = requests.get(download_url)  # scraping the PDF file from the URL\n",
    "            full_name = os.path.join(path + 'Data_Files_v2\\\\PDFs\\\\' + str(dataID) + '.pdf')\n",
    "            \n",
    "            with open(full_name, 'wb') as file:\n",
    "                file.write(r.content)\n",
    "            count = count + 1\n",
    "            \n",
    "            #print(dataID)\n",
    "        except:\n",
    "            # storing the error logs\n",
    "            error_urls.append(download_url)\n",
    "            error_dataIDs.append(dataID)\n",
    "            print(\"error with file {}\".format(row['File Name']))\n",
    "\n",
    "    # creating and and saving the error logs dataframe\n",
    "    df_scraping_errorlog = pd.DataFrame({'error_dataIDs': error_dataIDs,\n",
    "                                         'error_urls': error_urls\n",
    "                                         })\n",
    "    df_scraping_errorlog.to_csv(path + 'Data_Files_v2\\\\Error_Logs\\\\ScrapingPDFErrorLogs.csv', index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(\"Count of the PDFs to be downloaded: \", len(Index0))\n",
    "    print(\"Files successfully downloaded: \", count)\n",
    "    print(\"Count of Files with Errors: \", len(df_scraping_errorlog))\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files\n",
    "count = download_file(path, Index0)\n",
    "print(\"Files downloaded \", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dataID = []\n",
    "for index, row in Index0.iterrows():\n",
    "    pdf_dataID.append(row['Data ID'])\n",
    "    \n",
    "print(len(pdf_dataID))\n",
    "print(pdf_dataID[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extract Features for each page of the PDFs downloaded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_in_str(long_str, key):\n",
    "    count = 0\n",
    "    for word in long_str.split():\n",
    "        if key in word:\n",
    "            count = count + 1\n",
    "    return(count)\n",
    "#count_word_in_str(\"vibudh rocks dh dh ddh\",\"dh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_in_str(long_str, keys):\n",
    "    count = 0\n",
    "    for word in long_str.split():\n",
    "        for key in keys:\n",
    "            if key in word:\n",
    "                count = count + 1\n",
    "    return(count)\n",
    "#count_words_in_str(\"vibudh rocks dh dh ddh\",[\"dh\",\"vi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_of_imgblocks(imgblocks):\n",
    "    sum_areas = 0\n",
    "    for imgblock in imgblocks:\n",
    "        block_area = imgblock['width']* imgblock['height']\n",
    "        sum_areas = sum_areas + block_area\n",
    "    return(sum_areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'abc123def456ghi789ersuit834678 dhfuaiwhbui34tr234 zero0'\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "res = s.translate(remove_digits)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(dataIDs):\n",
    "    words_in_page = []\n",
    "    \n",
    "    scale = []\n",
    "    km_kilometers = []\n",
    "    m = []\n",
    "    metres = []\n",
    "    scale_grp = []\n",
    "    \n",
    "    legend = []\n",
    "    \n",
    "    figure = []\n",
    "    mapp = []\n",
    "    alignment_sheet = []\n",
    "    sheet = []\n",
    "    figure_grp = []\n",
    "    \n",
    "    north = []\n",
    "    n = []\n",
    "    north_grp = []\n",
    "    \n",
    "    dataID_pageNo = []\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    No_of_images = []\n",
    "    Area_of_images = []\n",
    "    cnt = 0\n",
    "    \n",
    "    dataID_l = []\n",
    "    page_no = []\n",
    "    paths_l = []\n",
    "    \n",
    "    error_files = []\n",
    "    i = 0\n",
    "    \n",
    "    for dataID in dataIDs:\n",
    "        pdf_path = path + '\\\\data\\\\raw\\\\pdfs\\\\' + str(dataID) + '.pdf'\n",
    "        print(pdf_path)\n",
    "        i = i+1 \n",
    "        print(\"File Starting: {}. PDF {} out of {}\".format(dataID, i, len(dataIDs)))     \n",
    "        \n",
    "        try:\n",
    "        \n",
    "            j = 0    \n",
    "            doc = fitz.open(pdf_path)             \n",
    "            #print(\"hey\", str(len(doc)))\n",
    "            for page in doc:  # iterate through the pages\n",
    "                print(page)\n",
    "                j = j+1 #Number of pages\n",
    "                print(j)\n",
    "                cnt = cnt + 1\n",
    "                print(cnt)\n",
    "                p = page.get_text(\"dict\")\n",
    "            \n",
    "                blocks = p[\"blocks\"]\n",
    "                imgblocks = [b for b in blocks if b[\"type\"] == 1]\n",
    "                No_of_images.append(len(imgblocks))\n",
    "                Area_of_images.append(area_of_imgblocks(imgblocks))\n",
    "                \n",
    "            \n",
    "                p = str(p).replace('<p>', '').replace('</p>','').replace(\".\",'').replace(\",\",'').replace('\"','').lower()\n",
    "                p = p.translate(remove_digits)\n",
    "                \n",
    "                words_lst = p.split()\n",
    "                \n",
    "                word_count = 0\n",
    "                big_words = \"\"\n",
    "                words = \"\"\n",
    "                #print(pdf_path)\n",
    "                for word in words_lst:\n",
    "                    words  = words + \" \" + word\n",
    "                    if len(word) > 3:\n",
    "                        word_count = word_count + 1\n",
    "                        big_words = big_words + \" \" + word\n",
    "                        \n",
    "                words_in_page.append(word_count)\n",
    "                #print(pdf_path)\n",
    "                \n",
    "                sc_grp = 0 \n",
    "                if \"scale\" in big_words:\n",
    "                    scale.append(count_word_in_str(big_words, \"scale\"))\n",
    "                    sc_grp = 1\n",
    "                else:\n",
    "                    scale.append(0)\n",
    "                    \n",
    "                if (\"kilometre\" in big_words or \"kilometer\" in big_words or \"km \" in p):\n",
    "                    km_kilometers.append(count_words_in_str(p, [\"kilometre\", \"kilometer\", \"km \"]))\n",
    "                    sc_grp = 1\n",
    "                else:\n",
    "                    km_kilometers.append(0)\n",
    "                    \n",
    "                if(\"m \" in p):\n",
    "                    m.append(count_word_in_str(p, \"m \"))\n",
    "                else:\n",
    "                    m.append(0)\n",
    "                    \n",
    "                if(\"metre\" in big_words or \"meter\" in big_words):\n",
    "                    metres.append(count_words_in_str(big_words, [\"meter\",\"metre\"]))\n",
    "                    sc_grp = 1\n",
    "                else:\n",
    "                    metres.append(0)\n",
    "                    \n",
    "                if sc_grp > 0:\n",
    "                    scale_grp.append(1)\n",
    "                else:\n",
    "                    scale_grp.append(0)\n",
    "                    \n",
    "                \n",
    "                \n",
    "                if \"legend\" in big_words:\n",
    "                    legend.append(count_word_in_str(big_words,\"legend\"))\n",
    "                else:\n",
    "                    legend.append(0)\n",
    "                \n",
    "                    \n",
    "                    \n",
    "                fig_grp = 0\n",
    "                if \"figure\" in big_words:\n",
    "                    figure.append(count_word_in_str(big_words,\"figure\"))\n",
    "                    fig_grp = 1\n",
    "                else:\n",
    "                    figure.append(0)\n",
    "                    \n",
    "                if \"map \" in p:\n",
    "                    mapp.append(count_word_in_str(p,\"map \"))\n",
    "                    fig_grp = 1\n",
    "                else:\n",
    "                    mapp.append(0)\n",
    "                      \n",
    "                if \"alignment sheet\" in big_words:\n",
    "                    alignment_sheet.append(1)\n",
    "                    fig_grp = 1\n",
    "                else:\n",
    "                    alignment_sheet.append(0)\n",
    "                    \n",
    "                if \"sheet\" in big_words:\n",
    "                    sheet.append(count_word_in_str(big_words,\"sheet\"))\n",
    "                    fig_grp = 1\n",
    "                else:\n",
    "                    sheet.append(0)\n",
    "                \n",
    "                if fig_grp > 0:\n",
    "                    figure_grp.append(1)\n",
    "                else:\n",
    "                    figure_grp.append(0)\n",
    "                \n",
    "                \n",
    "                   \n",
    "                if \"north\" in big_words:\n",
    "                    north.append(count_word_in_str(big_words, \"north\"))\n",
    "                    no_grp = 1\n",
    "                else:\n",
    "                    north.append(0)\n",
    "                    \n",
    "                if \"n\" in p:\n",
    "                    n.append(count_word_in_str(p, \" n \"))\n",
    "                    no_grp = 1\n",
    "                else:\n",
    "                    n.append(0)\n",
    "                          \n",
    "                dataID_l.append(dataID)\n",
    "                page_no.append(j)\n",
    "                paths_l.append(pdf_path)\n",
    "                \n",
    "        \n",
    "        except:\n",
    "            #if 1==0:\n",
    "            print(\"Error Found\")\n",
    "            error_files.append(dataID)\n",
    "            page_no.append(j)\n",
    "\n",
    "        \n",
    "    Features = pd.DataFrame({'scale' : scale, \n",
    "                           'km_kilometers' : km_kilometers, \n",
    "                           'm' : m, \n",
    "                           'metres' : metres, \n",
    "                           'scale_grp' : scale_grp, \n",
    "                           'legend' : legend, \n",
    "                           'figure' : figure, \n",
    "                           'mapp' : mapp, \n",
    "                           'alignment_sheet' : alignment_sheet, \n",
    "                           'sheet' : sheet, \n",
    "                           'figure_grp' : figure_grp, \n",
    "                           'north' : north, \n",
    "                           'n' : n, \n",
    "                           'words_in_page' : words_in_page,\n",
    "                            'No_of_images' : No_of_images, \n",
    "                            'Area_of_images' : Area_of_images,\n",
    "                           'dataID' : dataID_l, \n",
    "                            'pageNo': page_no, \n",
    "                             'paths_l': paths_l\n",
    "                           #'Y_class' : Y_class\n",
    "                           })\n",
    "    pages_ref = pd.DataFrame({'DataIDs': dataID_l,\n",
    "                            'pdf_paths' : paths_l, \n",
    "                            'Page_no': page_no})\n",
    "     \n",
    "       \n",
    "    #print(\"Total Number of pages processed: {}\".format(count))   \n",
    "    return Features, pages_ref, error_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"hello \" + 3\n",
    "X_df, pages_ref, error_files = extract_features(pdf_dataID) \n",
    "#Features\n",
    "#dataIDs\n",
    "#error_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total PDFs\", len(pdf_dataID))\n",
    "print(\"Total No. of Pages\", len(X_df))\n",
    "print(\"Total Error Files\", len(error_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_df))\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.to_csv(path + '\\\\data\\\\processed\\\\page_features\\\\all_features.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('berdi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5a57f819e391b0b4fe115ba667bdfe1c06f77f6806275f72654242525f22278a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
