{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling For All Content Types\n",
    "\n",
    "Use this notebook the final index file and label Alignment Sheets and Figures based on their title. This is the scenario where we have already labeled the tables. This notebook will be removed during the code review and cleaning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from nltk.util import ngrams\n",
    "from pathlib import Path\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.max_rows', None)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = Path('.').resolve().parents[1]\n",
    "print(ROOT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Load our list of keywords created in keywords.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(str(ROOT_PATH) + \"\\\\data\\\\processed\\\\keywords_pickle\\\\vc_keywords.pkl\", \"rb\") as f:\n",
    "    keywords = pickle.load(f)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keywords[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of VCs\n",
    "\n",
    "labels_list = ['Landscape, terrain, and weather', \n",
    "                'Soil',\n",
    "                'Plants',\n",
    "                'Water',\n",
    "                'Fish',\n",
    "                'Wetlands',\n",
    "                'Wildlife',\n",
    "                'Species at Risk',\n",
    "                'Greenhouse gas emissions',\n",
    "                'Air emissions',\n",
    "                'Noise',\n",
    "                'Electricity and electromagnetism',\n",
    "                'Proximity to people',\n",
    "                'Archaeological, paleontological, historical, and culturally significant sites and resources',\n",
    "                'Human access to boats and waterways',\n",
    "                'Indigenous land, water, and air use',\n",
    "                'Impact to social and cultural well-being',\n",
    "                'Impact to human health and viewscapes',\n",
    "                'Social, cultural, economic infrastructure and services',\n",
    "                'Economic Offsets and Impact',\n",
    "                'Environmental Obligations',\n",
    "                'Treaty and Indigenous Rights']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Final Index File\n",
    "\n",
    "This is where we load the 'final' index file so that we can label the figures and alignment sheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(str(ROOT_PATH) + '\\\\data\\\\interim\\\\Intermediate_Index_Files\\\\esa_index_with_table_text_no_labels.csv', encoding='utf-8-sig')\n",
    "df_fig_align = df[df['Content Type'].isin(['Figure', 'Alignment Sheet'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df), len(df_fig_align))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fig_align.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling For All Content Types\n",
    "\n",
    "Use this notebook the final index file and label Alignment Sheets and Figures based on their title. This is the scenario where we have already labeled the tables. This notebook will be removed during the code review and cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fig_align['text'] = df_fig_align['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fig_align.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "df_fig_align['text'] = df_fig_align['text'].apply(lambda x: BeautifulSoup(str(x), \"html.parser\").get_text(separator=' ')) # remove html tags\n",
    "df_fig_align['text'] = df_fig_align['text'].replace('[^a-zA-Z0-9 ]', ' ', regex=True) # remove all non-alpha-numeric characters\n",
    "df_fig_align['text'] = df_fig_align['text'].replace('\\w{25,}', ' ', regex=True)\n",
    "df_fig_align['text'] = df_fig_align['text'].replace('cid\\d+', ' ', regex=True)\n",
    "df_fig_align['text'] = df_fig_align['text'].replace(' s ', ' ', regex=True)\n",
    "df_fig_align['text'] = df_fig_align['text'].replace(' +', ' ', regex=True) # remove all extra spaces in text\n",
    "df_fig_align['text'] = df_fig_align['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fig_align['text'] = df_fig_align['text'].apply(lambda x: x[:30_000]) # making sure the text is not longer than 30k characters\n",
    "df_fig_align.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_texts = df_fig_align['text'].tolist()\n",
    "\n",
    "tokenized_table_texts = []\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for i, table_text in enumerate(table_texts):\n",
    "    processed_text = word_tokenize(table_text)\n",
    "    processed_text = [stemmer.stem(w) for w in processed_text if w not in stopwords.words(\"english\")]\n",
    "    table_ngram_list = []\n",
    "    for n in range(1, 7):\n",
    "        table_ngrams = list(ngrams(processed_text, n))\n",
    "        table_ngram_list.extend([\" \".join(table_gram) for table_gram in table_ngrams])\n",
    "\n",
    "    tokenized_table_texts.append(table_ngram_list)\n",
    "    \n",
    "print(tokenized_table_texts[0][0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_figTxt_pkl_path = str(ROOT_PATH / \"data\" / \"processed\" / \"keywords_pickle\" / \"tokenized_figure_alignment_sheets_texts.pkl\")\n",
    "with open(token_figTxt_pkl_path, 'wb') as f:\n",
    "    pickle.dump(tokenized_table_texts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(str(ROOT_PATH) + \"\\\\data\\\\processed\\\\keywords_pickle\\\\tokenized_figure_alignment_sheets_texts.pkl\", 'rb') as f:\n",
    "    tokenized_table_text = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_table_text[0][:10], '\\n', tokenized_table_text[-1][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in labels_list:\n",
    "    df_fig_align[f'{label}'] = 0\n",
    "    # df_f_as[f'{label} - Number of Matches'] = 0\n",
    "    # df_f_as[f'{label} Relevance'] = 0\n",
    "\n",
    "df_fig_align.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_with_keywords(i, table_text, keywords_for_label, label):\n",
    "    number_of_matches = len([word for word in keywords_for_label if word in table_text])\n",
    "    df_fig_align[f'{label}'][i] = number_of_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_total_sum_match_2_lists(list_1, list_2):\n",
    "    \"\"\"\n",
    "    This function takes two lists of strings and counts the number of total matches between the two lists, including duplicates.\n",
    "    The function returns the number of total matches.\n",
    "    \"\"\"\n",
    "    # Initialize the count variable.\n",
    "    count = 0\n",
    "    # Iterate over the first list.\n",
    "    for item in list_1:\n",
    "        # If the item is in the second list, increment the count variable.\n",
    "        if item in list_2:\n",
    "            count += 1\n",
    "    # Return the count variable.\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fig_align.text.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fig_align.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, table_text in zip(df_fig_align.index, tokenized_table_text):\n",
    "    for keywords_for_label, label in zip(keywords, labels_list):\n",
    "        number_of_matches = string_total_sum_match_2_lists(table_text, keywords_for_label)\n",
    "        df_fig_align[f'{label}'][i] = number_of_matches\n",
    "\n",
    "esa_fig_alignment_labeled_path = str(ROOT_PATH / \"data\" / \"processed\" / \"keywords_pickle\" / \"esa_index_ENG_fig_alignment_labeled.pkl\")\n",
    "with open(esa_fig_alignment_labeled_path, 'wb') as f:\n",
    "    pickle.dump(df_fig_align, f)\n",
    "\n",
    "df_fig_align.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop text and remerge index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fig_align.drop(columns=['text'], inplace=True)\n",
    "df_fig_align.drop(columns=['label'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fig_align.to_csv(str(ROOT_PATH) + '\\\\data\\\\interim\\\\Intermediate_Index_Files\\\\esa_figure_alignment_vec_labeled.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tables = pd.read_csv(str(ROOT_PATH) + '\\\\data\\\\interim\\\\Intermediate_Index_Files\\\\esa_tables_vec_labeled.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_esa_vecs = df_fig_align.append(df_tables, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 55) (473, 55) (502, 55)\n"
     ]
    }
   ],
   "source": [
    "print(df_fig_align.shape, df_tables.shape, df_esa_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_esa_vecs.drop(columns=['ID'], inplace=True)\n",
    "df_esa_vecs.insert(len(df_esa_vecs.columns), 'ID', range(21425, 21425 + len(df_esa_vecs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_esa_vecs.to_csv(str(ROOT_PATH) + '\\\\data\\\\interim\\\\Intermediate_Index_Files\\\\esa_vecs_labeled.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize match values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "max_value_list = []\n",
    "df_norm = df.copy()\n",
    "for content_type in ['Table', 'Figure', 'Alignment Sheet']:\n",
    "    df_content_type = df[df['Content Type'] == content_type]\n",
    "    for label in labels_list:\n",
    "        df_log = np.log2(df_content_type[f'{label}'].replace(0, np.nan) + 1)\n",
    "        max_vc = df_log.max()\n",
    "        updated_vc_col = df_log / max_vc * 100\n",
    "        updated_vc_col.replace(np.nan, 0, inplace=True)\n",
    "        df_content_type[f'{label}'] = np.ceil(updated_vc_col).astype(int)\n",
    "        max_value_list.append([content_type, label, max_vc])\n",
    "    df_norm.loc[df_content_type.index] = df_content_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(unique, counts) = np.unique(df_norm, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm_tables = df_norm[df_norm['Content Type'] == 'Table']\n",
    "df_norm_alignment_sheets = df_norm[df_norm['Content Type'] == 'Alignment Sheet']\n",
    "df_norm_figures = df_norm[df_norm['Content Type'] == 'Figure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 3, sharex='col', sharey='row')\n",
    "\n",
    "num = 1\n",
    "\n",
    "df_norm_tables.hist(column = labels_list[num], bins = 30, ax=ax[0], figsize=(10, 10))\n",
    "df_norm_alignment_sheets.hist(column = labels_list[num], bins = 30, ax=ax[1], figsize=(10, 10))\n",
    "df_norm_figures.hist(column = labels_list[num], bins = 30, ax=ax[2], figsize=(10, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_norm.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Alignment Sheet in Alignment Sheet rows with Figure in Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['Content Type'] == 'Alignment Sheet') & (df['Title'].str.contains('Figure')), 'Title'] = '(Alignment Sheet) ' + df.loc[(df['Content Type'] == 'Alignment Sheet'), 'Title']\n",
    "df.loc[(df['Content Type'] == 'Alignment Sheet') & (df['Title'].str.contains('FIGURE')), 'Title'] = '(Alignment Sheet) ' + df.loc[(df['Content Type'] == 'Alignment Sheet'), 'Title']\n",
    "df.loc[(df['Content Type'] == 'Alignment Sheet') & (df['Title'].str.contains('figure')), 'Title'] = '(Alignment Sheet) ' + df.loc[(df['Content Type'] == 'Alignment Sheet'), 'Title']\n",
    "\n",
    "df.loc[(df['Content Type'] == 'Alignment Sheet') & (df['Title'].str.contains('Table')), 'Title'] = '(Alignment Sheet) ' + df.loc[(df['Content Type'] == 'Alignment Sheet'), 'Title']\n",
    "df.loc[(df['Content Type'] == 'Alignment Sheet') & (df['Title'].str.contains('TABLE')), 'Title'] = '(Alignment Sheet) ' + df.loc[(df['Content Type'] == 'Alignment Sheet'), 'Title']\n",
    "df.loc[(df['Content Type'] == 'Alignment Sheet') & (df['Title'].str.contains('table')), 'Title'] = '(Alignment Sheet) ' + df.loc[(df['Content Type'] == 'Alignment Sheet'), 'Title']\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Alignment Sheet Titles for French Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Files_Path = str(Path().resolve().parents[1]) + '\\\\Data_Files\\\\'\n",
    "print(Data_Files_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_align = pd.read_csv(Data_Files_Path + \"Intermediate_Index_Files\\\\new_alignment_sheet_titles_for_translation_FR_fixed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_align.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add VCs to French Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fra = pd.read_csv(final_index_path + 'ESA_website_FRA.csv')\n",
    "\n",
    "df_fra.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grabbing the VC columns to put match numbers in French file\n",
    "df_fra.iloc[:, 29:51] = df.iloc[:, 29:51]\n",
    "df_fra.iloc[:, 29:51].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_update = df[df[\"ID Internal\"].isin(df_new_align[\"ID\"] + '_a_1')]['ID Internal'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_list = []\n",
    "for i, id in enumerate(ids_to_update):\n",
    "    id = id\n",
    "    idx = df_fra[df_fra[\"ID Internal\"] == id].index[0]\n",
    "    # print(idx)\n",
    "    idx_list.append(idx)\n",
    "    df_fra.loc[idx, 'Titre'] = df_new_align['Titre'][i]\n",
    "    # print(df_new_align['Titre'][i])\n",
    "df_fra.iloc[idx_list[20:25]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update the Alignment Sheet French Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fra.loc[(df_fra['Type de contenu'] == 'Carte-tracé') & (df_fra['Titre'].str.contains('Figure')), 'Titre'] = '(Carte-tracé) ' + df_fra.loc[(df_fra['Type de contenu'] == 'Carte-tracé'), 'Titre']\n",
    "df_fra.loc[(df_fra['Type de contenu'] == 'Carte-tracé') & (df_fra['Titre'].str.contains('FIGURE')), 'Titre'] = '(Carte-tracé) ' + df_fra.loc[(df_fra['Type de contenu'] == 'Carte-tracé'), 'Titre']\n",
    "df_fra.loc[(df_fra['Type de contenu'] == 'Carte-tracé') & (df_fra['Titre'].str.contains('figure')), 'Titre'] = '(Carte-tracé) ' + df_fra.loc[(df_fra['Type de contenu'] == 'Carte-tracé'), 'Titre']\n",
    "\n",
    "df_fra.loc[(df_fra['Type de contenu'] == 'Carte-tracé') & (df_fra['Titre'].str.contains('Tableau')), 'Titre'] = '(Carte-tracé) ' + str(df_fra.loc[(df_fra['Type de contenu'] == 'Carte-tracé'), 'Titre'])\n",
    "df_fra.loc[(df_fra['Type de contenu'] == 'Carte-tracé') & (df_fra['Titre'].str.contains('TABLEAU')), 'Titre'] = '(Carte-tracé) ' + str(df_fra.loc[(df_fra['Type de contenu'] == 'Carte-tracé'), 'Titre'])\n",
    "df_fra.loc[(df_fra['Type de contenu'] == 'Carte-tracé') & (df_fra['Titre'].str.contains('tableau')), 'Titre'] = '(Carte-tracé) ' + str(df_fra.loc[(df_fra['Type de contenu'] == 'Carte-tracé'), 'Titre'])\n",
    "df_fra.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fra.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_duplicate_as = ['1059614_45_a_1',\n",
    "'2392795_110_a_1',\n",
    "'2393296_31_a_1',\n",
    "'3334565_29_a_1',\n",
    "'3334565_31_a_1',\n",
    "'3334565_43_a_1',\n",
    "'3334565_44_a_1',\n",
    "'3337530_34_a_1',\n",
    "'3337530_35_a_1',\n",
    "'3337530_6_a_1',\n",
    "'3340309_17_a_1',\n",
    "'3340309_21_a_1',\n",
    "'3341938_15_a_1',\n",
    "'3341938_25_a_1',\n",
    "'3341938_31_a_1',\n",
    "'3341938_32_a_1',\n",
    "'3341938_33_a_1',\n",
    "'3342531_19_a_1',\n",
    "'3342531_4_a_1',\n",
    "'3342531_45_a_1',\n",
    "'3891804_134_a_1',\n",
    "'464812_45_a_1',\n",
    "'464812_46_a_1',\n",
    "'464812_47_a_1',\n",
    "'464812_48_a_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in remove_duplicate_as:\n",
    "    df.drop(df[df['ID Internal'] == id].index[0], inplace=True)\n",
    "    df_fra.drop(df_fra[df_fra['ID Internal'] == id].index[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"ID Internal\"] == remove_duplicate_as[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Checks\n",
    "\n",
    "Let's have a look at all the rows where there were no matches for any of the VCs. We want to make sure this is only because none of the text matched with any of the keywords in our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running this cell, open up an index file with the text content and search for the table title to see if it makes sense that such a row has 0 matches.\n",
    "\n",
    "df[df.iloc[:, 29:51].sum(axis=1) == 0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Labeled Index File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "current_time = time.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "print(current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_index_path = str(Path().resolve().parents[1]) + '\\\\Output_Files\\\\final_index_files\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(save_index_path + f'ESA_website_ENG_{current_time}.csv', encoding='utf-8-sig')\n",
    "df_fra.to_csv(save_index_path + f'ESA_website_FRA_{current_time}.csv', encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('berdi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a57f819e391b0b4fe115ba667bdfe1c06f77f6806275f72654242525f22278a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
